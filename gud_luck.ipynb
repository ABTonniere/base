{
 "cells": [
  {
   "cell_type": "code",
   "id": "fbc121e30a2defb3",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "697ba68f2aa0fe1f",
   "metadata": {},
   "source": [
    "DATASET_PATH = \"/Users/baptiste/cours/base/spam.csv\"\n",
    "\n",
    "# Tested multiple value 0.2 seems good\n",
    "TEST_SIZE = 0.2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "758041dcab6bad3a",
   "metadata": {},
   "source": [
    "df = pd.read_csv(DATASET_PATH, encoding=\"ISO-8859-1\")\n",
    "print(df.head())\n",
    "print(df.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Label distribution :\")\n",
    "print(df[\"label\"].value_counts())\n",
    "\n",
    "print(\"Example of 'spam' message :\")\n",
    "print(df[df[\"label\"] == \"spam\"].head(2).values)\n",
    "\n",
    "print(\"Example of 'ham' message:\")\n",
    "print(df[df[\"label\"] == \"ham\"].head(2).values)"
   ],
   "id": "9ac1bd2102d6a891",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "##### Remove duplicates and null values #####\n",
    "\n",
    "print(f\"Before: {len(df)}\")\n",
    "\n",
    "df = df.drop_duplicates(subset=[\"message\"])\n",
    "\n",
    "print(f\"After: {len(df)}\")"
   ],
   "id": "3fbc5c7e4560bfba",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dc10f4ca30c525ff",
   "metadata": {},
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "df[\"message_clean\"] = df[\"message\"].str.lower()\n",
    "df[\"message_clean\"] = df[\"message_clean\"].apply(lambda x: re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", x))\n",
    "df[\"message_clean\"] = df[\"message_clean\"].apply(lambda x: re.sub(r\"\\S+@\\S+\", \"\", x))\n",
    "df[\"message_clean\"] = df[\"message_clean\"].apply(lambda x: re.sub(r\"\\d+\", \"\", x))\n",
    "df[\"message_clean\"] = df[\"message_clean\"].apply(lambda x: x.translate(str.maketrans(\"\", \"\", string.punctuation)))\n",
    "df[\"message_clean\"] = df[\"message_clean\"].apply(lambda x: \" \".join(x.split()))\n",
    "\n",
    "print(f\"Before: {df['message'].iloc[0]}\")\n",
    "print(f\"After: {df['message_clean'].iloc[0]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a472700633e90698",
   "metadata": {},
   "source": [
    "df[\"message_length\"] = df[\"message\"].apply(len)\n",
    "df[\"word_count\"] = df[\"message\"].apply(lambda x: len(x.split()))\n",
    "df[\"avg_word_length\"] = df[\"message_length\"] / df[\"word_count\"]\n",
    "\n",
    "df[\"caps_count\"] = df[\"message\"].apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "df[\"caps_ratio\"] = df[\"caps_count\"] / df[\"message_length\"]\n",
    "\n",
    "df[\"special_chars\"] = df[\"message\"].apply(lambda x: sum(1 for c in x if c in \"!?$â‚¬Â£%\"))\n",
    "\n",
    "print(\"features added :\")\n",
    "print(df[[\"message_length\", \"word_count\", \"caps_ratio\", \"special_chars\"]].describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b31a749762d0e28e",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Tested multiple value 5000 is good\n",
    "# ngram with (1, 2) works\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=2, max_df=0.95, stop_words=\"english\")\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(df[\"message_clean\"])\n",
    "print(f\"Shape TF-IDF: {X_tfidf.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "afbc99702431c3dc",
   "metadata": {},
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "numerical_features = [\n",
    "    \"message_length\",\n",
    "    \"word_count\",\n",
    "    \"avg_word_length\",\n",
    "    \"caps_ratio\",\n",
    "    \"special_chars\",\n",
    "]\n",
    "\n",
    "X_numerical = df[numerical_features].values\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_numerical_scaled = scaler.fit_transform(X_numerical)\n",
    "\n",
    "X_numerical_sparse = csr_matrix(X_numerical_scaled)\n",
    "X_combined = hstack([X_tfidf, X_numerical_sparse])\n",
    "\n",
    "print(f\"Shape finale: {X_combined.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"label\"])\n",
    "\n",
    "print(f\"Classes: {le.classes_}\")\n",
    "print(f\"Distribution: {np.bincount(y)}\")"
   ],
   "id": "9b4d7f6314e0d5ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=TEST_SIZE, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "import numpy as np\n",
    "\n",
    "classes, counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "print(\"Classes :\", classes)"
   ],
   "id": "3f46bd31f9a13076",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f4ea42194db2442a",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"=== Logistic Regression ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=le.classes_))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17c431e0e9c84a50",
   "metadata": {},
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\",\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"=== XGBoost ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=le.classes_))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bf21074efa59e12",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"C\": [0.1, 0.5, 1.0, 5.0],\n",
    "    \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"solver\": [\"liblinear\"],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring=\"f1\", n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best params: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "best_model = grid_search.best_estimator_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ec639c3c96f963d3",
   "metadata": {},
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "y_pred_final = best_model.predict(X_test)\n",
    "y_proba_final = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_final):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba_final):.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_final))\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_final, target_names=le.classes_))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "54e9e21027317c7e",
   "metadata": {},
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba_final)\n",
    "auc_score = roc_auc_score(y_test, y_proba_final)\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_proba_final)\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=(\"ROC Curve\", \"Precision-Recall Curve\"))\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=fpr, y=tpr, name=f\"ROC (AUC = {auc_score:.3f})\", mode=\"lines\"),\n",
    "    row=1,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=recall, y=precision, name=\"Precision-Recall\", mode=\"lines\"),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"False Positive Rate\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"True Positive Rate\", row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Recall\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Precision\", row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=500, width=1000, title_text=\"Model Evaluation Metrics\", showlegend=True)\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "972385349d225c47",
   "metadata": {},
   "source": [
    "feature_names = vectorizer.get_feature_names_out().tolist() + numerical_features\n",
    "\n",
    "coefs = best_model.coef_[0]\n",
    "top_positive = np.argsort(coefs)[-10:]\n",
    "top_negative = np.argsort(coefs)[:10]\n",
    "\n",
    "print(\"Top 10 features SPAM:\")\n",
    "for idx in reversed(top_positive):\n",
    "    print(f\"  {feature_names[idx]}: {coefs[idx]:.4f}\")\n",
    "\n",
    "print(\"Top 10 features HAM:\")\n",
    "for idx in top_negative:\n",
    "    print(f\"  {feature_names[idx]}: {coefs[idx]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f3e799bb7cb638be",
   "metadata": {},
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "with open(\"models/model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "with open(\"models/vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "with open(\"models/scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "with open(\"models/label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "\n",
    "print(\"OK\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4cb06182f3554122",
   "metadata": {},
   "source": [
    "def predict_spam(text: str) -> tuple[str, float]:\n",
    "    text_clean = text.lower()\n",
    "    text_clean = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text_clean)\n",
    "    text_clean = re.sub(r\"\\S+@\\S+\", \"\", text_clean)\n",
    "    text_clean = re.sub(r\"\\d+\", \"\", text_clean)\n",
    "    text_clean = text_clean.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text_clean = \" \".join(text_clean.split())\n",
    "\n",
    "    X_tfidf = vectorizer.transform([text_clean])\n",
    "\n",
    "    text_length = len(text)\n",
    "    word_count = len(text.split())\n",
    "    avg_word_length = text_length / word_count if word_count > 0 else 0\n",
    "    caps_count = sum(1 for c in text if c.isupper())\n",
    "    caps_ratio = caps_count / text_length if text_length > 0 else 0\n",
    "    special_chars = sum(1 for c in text if c in \"!?$â‚¬Â£%\")\n",
    "\n",
    "    X_numerical = [[text_length, word_count, avg_word_length, caps_ratio, special_chars]]\n",
    "    X_numerical_scaled = scaler.transform(X_numerical)\n",
    "\n",
    "    X_combined = hstack([X_tfidf, csr_matrix(X_numerical_scaled)])\n",
    "\n",
    "    pred = best_model.predict(X_combined)[0]\n",
    "    proba = best_model.predict_proba(X_combined)[0]\n",
    "\n",
    "    label = le.inverse_transform([pred])[0]\n",
    "    confidence = max(proba) * 100\n",
    "\n",
    "    return label, confidence\n",
    "\n",
    "\n",
    "test_messages = [\n",
    "    \"FREE!!! You have won a $1000 Walmart gift card! Click here to claim NOW!!!\",\n",
    "    \"Hey, are we still meeting for coffee tomorrow at 3pm?\",\n",
    "    \"URGENT: Your account will be suspended. Verify your details immediately.\",\n",
    "    \"Thanks for your help with the project yesterday. Really appreciated it!\",\n",
    "    \"Congratulations! You've been selected for a FREE iPhone 15! Call 0800-123-456\",\n",
    "    \"Can you send me the meeting notes when you get a chance?\",\n",
    "]\n",
    "\n",
    "for msg in test_messages:\n",
    "    label, confidence = predict_spam(msg)\n",
    "    emoji = \"ðŸš«\" if label == \"spam\" else \"âœ…\"\n",
    "    print(f\"{emoji} [{label:4}] ({confidence:.1f}%) {msg[:50]}...\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4b70e7350c8e870d",
   "metadata": {},
   "source": [
    "# # Ancien code de preprocessing - ne pas utiliser\n",
    "# def old_preprocess(text):\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r'[^a-z\\s]', '', text)\n",
    "#     return text\n",
    "\n",
    "# # Essai avec SVM - trop lent, abandonnÃ©\n",
    "# from sklearn.svm import SVC\n",
    "# svm_model = SVC(kernel='rbf', probability=True)\n",
    "# svm_model.fit(X_train, y_train)\n",
    "\n",
    "# # Test avec CountVectorizer au lieu de TF-IDF\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# cv = CountVectorizer(max_features=5000)\n",
    "# X_cv = cv.fit_transform(df['text_clean'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "197b0edb9dad53e4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
